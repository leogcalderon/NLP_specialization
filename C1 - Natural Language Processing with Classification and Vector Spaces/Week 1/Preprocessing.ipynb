{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk                                \n",
    "from nltk.corpus import twitter_samples    \n",
    "import matplotlib.pyplot as plt            \n",
    "import random\n",
    "\n",
    "import re                             \n",
    "import string          \n",
    "\n",
    "from nltk.corpus import stopwords        \n",
    "from nltk.stem import PorterStemmer   \n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tweets = twitter_samples.strings(\"positive_tweets.json\")\n",
    "negative_tweets = twitter_samples.strings(\"negative_tweets.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[90mPositive tweet:  \u001b[92m@Bosslogic @amellywood @CW_Arrow @ARROWwriters Thank you! :-)\n",
      "\u001b[90mNegative tweet:  \u001b[91mcries bc i miss mingming so much :-(\n"
     ]
    }
   ],
   "source": [
    "print(\"\\033[90mPositive tweet: \",\"\\033[92m\" + positive_tweets[25])\n",
    "print(\"\\033[90mNegative tweet: \", \"\\033[91m\" + negative_tweets[25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example tweet: My beautiful sunflowers on a sunny Friday morning off :) #sunflowers #favourites #happy #Friday off… https://t.co/3tfYom0N1i\n"
     ]
    }
   ],
   "source": [
    "tweet = positive_tweets[2277]\n",
    "print(\"Example tweet:\", tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My beautiful sunflowers on a sunny Friday morning off :) #sunflowers #favourites #happy #Friday off… https://t.co/3tfYom0N1i\n",
      "My beautiful sunflowers on a sunny Friday morning off :) sunflowers favourites happy Friday off… \n"
     ]
    }
   ],
   "source": [
    "print(tweet)\n",
    "tweet2 = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "tweet2 = re.sub(r'#', '', tweet2)\n",
    "print(tweet2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my',\n",
       " 'beautiful',\n",
       " 'sunflowers',\n",
       " 'on',\n",
       " 'a',\n",
       " 'sunny',\n",
       " 'friday',\n",
       " 'morning',\n",
       " 'off',\n",
       " ':)',\n",
       " 'sunflowers',\n",
       " 'favourites',\n",
       " 'happy',\n",
       " 'friday',\n",
       " 'off',\n",
       " '…']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,reduce_len=True)\n",
    "tweet_tokens = tokenizer.tokenize(tweet2)\n",
    "tweet_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "stopwords_english = stopwords.words('english') \n",
    "print(stopwords_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet with stop words removed:  ['beautiful', 'sunflowers', 'sunny', 'friday', 'morning', ':)', 'sunflowers', 'favourites', 'happy', 'friday', '…']\n",
      "Stop words removed:  ['my', 'on', 'a', 'off', 'off']\n"
     ]
    }
   ],
   "source": [
    "tweet_clean = []\n",
    "deleted = []\n",
    "for word in tweet_tokens:\n",
    "    if word not in stopwords_english:\n",
    "        tweet_clean.append(word)\n",
    "    else:\n",
    "        deleted.append(word)\n",
    "print(\"Tweet with stop words removed: \",tweet_clean)\n",
    "print(\"Stop words removed: \", deleted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['beauti',\n",
       " 'sunflow',\n",
       " 'sunni',\n",
       " 'friday',\n",
       " 'morn',\n",
       " ':)',\n",
       " 'sunflow',\n",
       " 'favourit',\n",
       " 'happi',\n",
       " 'friday',\n",
       " '…']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "tweet_stemmed = []\n",
    "for word in tweet_clean:\n",
    "    stem_word = stemmer.stem(word)\n",
    "    tweet_stemmed.append(stem_word)\n",
    "tweet_stemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All in one function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp import preprocess_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['beauti',\n",
       " 'sunflow',\n",
       " 'sunni',\n",
       " 'friday',\n",
       " 'morn',\n",
       " ':)',\n",
       " 'sunflow',\n",
       " 'favourit',\n",
       " 'happi',\n",
       " 'friday',\n",
       " '…']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_tweet(tweet)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
