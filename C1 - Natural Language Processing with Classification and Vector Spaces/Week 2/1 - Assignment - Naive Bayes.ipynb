{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords, twitter_samples\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nlp import preprocess_tweet, get_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "X_train = positive_tweets[:4000] + negative_tweets[:4000] \n",
    "X_test = positive_tweets[4000:] + negative_tweets[4000:]\n",
    "\n",
    "y_train = np.append(np.ones((4000)), np.zeros((4000)))\n",
    "y_test = np.append(np.ones((1000)), np.zeros((1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict = get_freq(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Log Prior\n",
    "The prior probability represents the underlying probability in the target population that a tweet is positive versus negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$D = document$$\n",
    "\n",
    "$$D_{neg} = negative{\\:}document$$\n",
    "\n",
    "$$D_{pos} = positive{\\:}document$$\n",
    "\n",
    "$$P(D_{neg}) = \\frac{D_{neg}}{D}$$\n",
    "                                     \n",
    "                                     \n",
    "$$Prior Ratio = \\frac{P(D_{pos})}{P(D_{neg})}$$\n",
    "\n",
    "\n",
    "$$logPrior = log(Prior Ratio) = log(\\frac{P(D_{pos})}{P(D_{neg})})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dataset the logPrior is 0, because we have the same number of (+) and (-) tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Positive and Negative Probability of a Word "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$freq_{pos} = word{\\:}count{\\:}in{\\:}the{\\:}positive{\\:}class{\\:}$$\n",
    "\n",
    "$$freq_{neg} = word{\\:}count{\\:}in{\\:}the{\\:}negative{\\:}class{\\:}$$\n",
    "\n",
    "$$N_{pos} = number{\\:}of{\\:}possitive{\\:}classes$$\n",
    "\n",
    "$$N_{neg} = number{\\:}of{\\:}negative{\\:}classes$$\n",
    "\n",
    "$$V = number{\\:}of{\\:}words$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(W_{pos}) = \\frac{freq_{pos}+1}{N_{pos}+V}$$\n",
    "\n",
    "$$P(W_{neg}) = \\frac{freq_{neg}+1}{N_{neg}+V}$$\n",
    "\n",
    "$$Log{\\:}likelihood = log(\\frac{P(W_{pos})}{P(W_{neg})})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p_{W} = logPrior + Log{\\:}likelihood$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_words(tweets, labels, freq):\n",
    "    '''\n",
    "    returns the p for each word found in the tweets\n",
    "    '''\n",
    "    '''\n",
    "    Preprocess\n",
    "    '''\n",
    "    tweets = [preprocess_tweet(tweet) for tweet in tweets]\n",
    "    '''\n",
    "    Log Prior parameters\n",
    "    '''\n",
    "    d = len(labels)\n",
    "    d_pos = sum(labels)\n",
    "    d_neg = len(labels) - d_pos\n",
    "    log_prior = np.log((d_pos/d)/(d_neg/d))\n",
    "    \n",
    "    '''\n",
    "    Log Likelihood parameters\n",
    "    '''\n",
    "    N_pos = 0\n",
    "    N_neg = 0\n",
    "    \n",
    "    for pair in freq.keys():\n",
    "        if pair[1] == 1:\n",
    "            N_pos += 1\n",
    "        else:\n",
    "            N_neg += 1\n",
    "        \n",
    "    V = N_pos + N_neg\n",
    "    \n",
    "    vocab = [pair[0] for pair in freq.keys()]\n",
    "    \n",
    "    p = []\n",
    "\n",
    "    for word in vocab:\n",
    "        if (word,1) in freq.keys():\n",
    "            p_word_pos = ((freq[(word,1)] + 1) + 1) / (N_pos + V)\n",
    "        else:\n",
    "            p_word_pos = ((0 + 1) + 1) / (N_pos + V)\n",
    "            \n",
    "        if (word,0) in freq.keys():\n",
    "            p_word_neg = ((freq[(word,0)] + 1) + 1) / (N_neg + V)\n",
    "        else:\n",
    "            p_word_neg = ((0 + 1) + 1) / (N_pos + V)\n",
    "            \n",
    "        p.append(np.log(p_word_pos/p_word_neg) + log_prior)\n",
    "        \n",
    "           \n",
    "    return p, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, vocab = p_words(X_train,y_train,train_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word followfriday has a log likelihood of 2.5257286443082556\n"
     ]
    }
   ],
   "source": [
    "print(\"The word\",vocab[0],\"has a log likelihood of\", p[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_naive_bayes(tweets,labels,p,vocab):\n",
    "    '''\n",
    "    predict if a tweets is positive or negative with the p and vocab already trained\n",
    "    '''\n",
    "    '''\n",
    "    Preprocess\n",
    "    '''\n",
    "    tweets = [preprocess_tweet(tweet) for tweet in tweets]\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    for tweet in tweets:\n",
    "        \n",
    "        p_tweet = 0\n",
    "        \n",
    "        for word in tweet:\n",
    "            if word in vocab:\n",
    "                idx = vocab.index(word)\n",
    "                p_tweet += p[idx]\n",
    "            \n",
    "        if p_tweet > 0:\n",
    "            predictions.append(1)\n",
    "        else:\n",
    "            predictions.append(0)\n",
    "    \n",
    "    accuracy = sum([predictions[i] == labels[i] for i in range(len(predictions))]) / len(predictions)\n",
    "    \n",
    "    return predictions, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, accuracy = test_naive_bayes(X_test,y_test,p,vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.997"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tweets = [\"I hated this movie, it was horrible.\",\n",
    "               \"He seems to be very happy, I am glad about that\",\n",
    "               \"I can't stand pop music, it makes me angry\",\n",
    "               \"I am very happy because I think this Naive Bayes classificator is going to work very well\"]\n",
    "test_labels = np.array([0,1,0,1])\n",
    "predictions, accuracy = test_naive_bayes(test_tweets,test_labels,p,vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 1, 0, 1], 1.0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions, accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
