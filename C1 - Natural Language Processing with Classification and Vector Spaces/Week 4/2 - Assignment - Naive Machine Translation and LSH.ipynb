{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import pickle\n",
    "import string\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "from nltk.corpus import twitter_samples\n",
    "from nlp import cosine,get_freq,preprocess_tweet\n",
    "from os import getcwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The word embeddings data for English and French words\n",
    "\n",
    "Write a program that translates English to French."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_embeddings_subset = pickle.load(open(\"en_embeddings.p\", \"rb\"))\n",
    "fr_embeddings_subset = pickle.load(open(\"fr_embeddings.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dict(file_name):\n",
    "    \n",
    "    file = pd.read_csv(file_name, delimiter=' ')\n",
    "    dicc = {}\n",
    "    \n",
    "    for i in range(len(file)):\n",
    "        \n",
    "        en = file.loc[i][0]\n",
    "        fr = file.loc[i][1]\n",
    "        dicc[en] = fr\n",
    "\n",
    "    return dicc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_fr_train = get_dict('en-fr.train.txt')\n",
    "en_fr_test = get_dict('en-fr.test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('la', 'maison')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_fr_train[\"the\"] , en_fr_train[\"house\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://github.com/ijelliti/Deeplearning.ai-Natural-Language-Processing-Specialization/raw/b737ae959c890506978a8987ccba195a2ee80c4f/1%20-%20Natural%20Language%20Processing%20with%20Classification%20and%20Vector%20Spaces/Labs/Week%204/X_to_Y.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating matrixes X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matrices(en_fr, french_vecs, english_vecs):\n",
    "\n",
    "    X_l = list()\n",
    "    Y_l = list()\n",
    "    \n",
    "    english_set = english_vecs.keys()\n",
    "    french_set = french_vecs.keys()\n",
    "    french_words = set(en_fr.values())\n",
    "\n",
    "    for en_word, fr_word in en_fr.items():\n",
    "        if fr_word in french_set and en_word in english_set:\n",
    "\n",
    "            en_vec = english_vecs[en_word]\n",
    "            fr_vec = french_vecs[fr_word]\n",
    "            X_l.append(en_vec)\n",
    "            Y_l.append(fr_vec)\n",
    "\n",
    "\n",
    "    X =  np.vstack(X_l)\n",
    "    Y = np.vstack(Y_l)\n",
    "\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = get_matrices(en_fr_train,fr_embeddings_subset,en_embeddings_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translations\n",
    "Given dictionaries of English and French word embeddings you will create a transformation matrix R\n",
    "\n",
    "- Given an English word embedding, $\\mathbf{e}$, you can multiply $\\mathbf{eR}$ to get a new word embedding $\\mathbf{f}$.\n",
    "- Both $\\mathbf{e}$ and $\\mathbf{f}$ are row vectors.\n",
    "- You can then compute the nearest neighbors to $\\mathbf{f}$ in the french embeddings and recommend the word that is most similar to the transformed word embedding.\n",
    "\n",
    "\n",
    "                            Find a matrix R that minimizes the following equation. ( with gradient descent)\n",
    "\n",
    "$\\arg \\min _{\\mathbf{R}}\\| \\mathbf{X R} - \\mathbf{Y}\\|_{F}\\tag{1} $\n",
    "\n",
    "$R := R - {\\alpha}\\frac{d}{dR} \\tag{2}$\n",
    "\n",
    "\n",
    "$\\frac{d}{dR} = \\frac{2}{m}X^{T}(XR-Y)\\tag{3}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(X, Y, R):\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    loss = (1/m)*np.sum(np.square(np.dot(X,R) - Y))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, Y, R):\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    gradient = (2/m)*np.dot(X.T,np.dot(X,R) - Y)\n",
    "    \n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, Y, train_steps=100, learning_rate=0.0003):\n",
    "    \n",
    "    R = np.random.rand(X.shape[1],X.shape[1])\n",
    "    hist = []\n",
    "    \n",
    "    for i in range(train_steps):\n",
    "        cost = compute_loss(X, Y, R)\n",
    "        gradient = compute_gradient(X, Y, R)\n",
    "        R -= learning_rate*gradient\n",
    "        hist.append(cost)\n",
    "        \n",
    "    return R, hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_train, hist = gradient_descent(X_train, Y_train, train_steps=400, learning_rate=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(hist)\n",
    "plt.title(\"Cost history\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_neighbor(v, candidates, k=1):\n",
    "    \n",
    "    s = []\n",
    "    \n",
    "    for row in candidates:\n",
    "        s.append(cosine(v, row))\n",
    "        \n",
    "    s_idx_sorted = np.argsort(s)\n",
    "    closest_idx = s_idx_sorted[-k:]\n",
    "    \n",
    "    return closest_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_vocabulary(X, Y, R):\n",
    "        \n",
    "    predictions = np.dot(X,R)\n",
    "    \n",
    "    correct = 0\n",
    "    total = len(predictions)\n",
    "    \n",
    "    for i in range(len(predictions)):\n",
    "        pred_idx = nearest_neighbor(predictions[i],Y)\n",
    "        \n",
    "        if pred_idx == i:\n",
    "            correct += 1\n",
    "    \n",
    "    accuracy = correct/total\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, Y_val = get_matrices(en_fr_test, fr_embeddings_subset, en_embeddings_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5521557719054242"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_vocabulary(X_val, Y_val, R_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSH and document search \n",
    "In this part of the assignment, you will implement a more efficient version of k-nearest neighbors using locality sensitive hashing. You will then apply this to document search.\n",
    "\n",
    "    Process the tweets and represent each tweet as a vector (represent a document with a vector embedding).\n",
    "    Use locality sensitive hashing and k nearest neighbors to find tweets that are similar to a given tweet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "all_tweets = all_positive_tweets + all_negative_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_embedding(tweet, en_embedding):\n",
    "    tweet = preprocess_tweet(tweet)\n",
    "    doc_embedding = np.zeros(300)\n",
    "    for word in tweet:\n",
    "        doc_embedding += en_embedding.get(word,0)\n",
    "    return doc_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_vecs(tweets, en_embeddings):\n",
    "    \n",
    "    tweets_dic = {}\n",
    "    \n",
    "    matrix_tweets = []\n",
    "    \n",
    "    for i, tweet in enumerate(tweets):\n",
    "        tweet_embedding = get_document_embedding(tweet, en_embeddings)\n",
    "        tweets_dic[i] = tweet_embedding\n",
    "        matrix_tweets.append(tweet_embedding)\n",
    "    \n",
    "    matrix = np.vstack(matrix_tweets)\n",
    "    \n",
    "    return matrix, tweets_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_vecs, ind2Tweet = get_document_vecs(all_tweets, en_embeddings_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have a vector of dimension (m,d) where m is the number of tweets (10,000) and d is the dimension of the embeddings (300). Now you will input a tweet, and use cosine similarity to see which tweet in our corpus is similar to your tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tweet = \"This doesn't work like I expected\"\n",
    "tweet_embedding = get_document_embedding(my_tweet, en_embeddings_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@PetiteMistress DO IT! I want to start one for making small games, but I feel like I need to get a jump start before asking for support :(\n"
     ]
    }
   ],
   "source": [
    "idx = np.argmax(cosine(document_vecs, tweet_embedding))\n",
    "print(all_tweets[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the most similar tweets with LSH\n",
    "You will now implement locality sensitive hashing (LSH) to identify the most similar tweet.\n",
    "\n",
    "    Instead of looking at all 10,000 vectors, you can just search a subset to find its nearest neighbors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_planes = 10\n",
    "n_universes = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$hash =\\sum_{i=1}^{n}(2^{i}.h_{i})\\tag{1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "planes_l = [np.random.normal(size=(300, n_planes)) for _ in range(n_universes)]\n",
    "# Creates 25 universes with 10 random planes each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_value_of_vector(v, planes):\n",
    "    \"\"\"Create a hash for a vector; hash_id says which random hash to use.\n",
    "    Input:\n",
    "        - v:  vector of tweet. It's dimension is (1, N_DIMS)\n",
    "        - planes: matrix of dimension (N_DIMS, N_PLANES) - the set of planes that divide up the region\n",
    "    Output:\n",
    "        - res: a number which is used as a hash for your vector\n",
    "\n",
    "    \"\"\"\n",
    "    hash_v = 0\n",
    "    sign = np.sign(np.dot(v,planes))\n",
    "    bit = (sign >=0)\n",
    "    bit = np.squeeze(bit)\n",
    "    \n",
    "    for i,h in enumerate(bit):\n",
    "        hash_v += np.power(2,i)*bit[i]\n",
    "    \n",
    "    return hash_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_hash_table(vecs, planes):\n",
    "\n",
    "    num_of_planes = planes.shape[1]\n",
    "    num_buckets = 2**num_of_planes\n",
    "\n",
    "    hash_table = {i:[] for i in range(num_buckets)}\n",
    "    id_table = {i:[] for i in range(num_buckets)}\n",
    "\n",
    "    for i, v in enumerate(vecs):\n",
    "\n",
    "        h = hash_value_of_vector(v,planes)\n",
    "        hash_table[h].append(v)\n",
    "        id_table[h].append(i)\n",
    "\n",
    "    return hash_table, id_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on hash universe #: 0\n",
      "working on hash universe #: 1\n",
      "working on hash universe #: 2\n",
      "working on hash universe #: 3\n",
      "working on hash universe #: 4\n",
      "working on hash universe #: 5\n",
      "working on hash universe #: 6\n",
      "working on hash universe #: 7\n",
      "working on hash universe #: 8\n",
      "working on hash universe #: 9\n",
      "working on hash universe #: 10\n",
      "working on hash universe #: 11\n",
      "working on hash universe #: 12\n",
      "working on hash universe #: 13\n",
      "working on hash universe #: 14\n",
      "working on hash universe #: 15\n",
      "working on hash universe #: 16\n",
      "working on hash universe #: 17\n",
      "working on hash universe #: 18\n",
      "working on hash universe #: 19\n",
      "working on hash universe #: 20\n",
      "working on hash universe #: 21\n",
      "working on hash universe #: 22\n",
      "working on hash universe #: 23\n",
      "working on hash universe #: 24\n"
     ]
    }
   ],
   "source": [
    "# Creating the hashtables\n",
    "hash_tables = []\n",
    "id_tables = []\n",
    "for universe_id in range(n_universes):  # there are 25 hashes\n",
    "    print('working on hash universe #:', universe_id)\n",
    "    planes = planes_l[universe_id]\n",
    "    hash_table, id_table = make_hash_table(document_vecs, planes)\n",
    "    hash_tables.append(hash_table)\n",
    "    id_tables.append(id_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def approximate_knn(doc_id, v, planes_l, k=1, num_universes_to_use=n_universes):\n",
    "    \n",
    "    vecs_to_consider_l = list()\n",
    "    ids_to_consider_l = list()\n",
    "    ids_to_consider_set = set()\n",
    "\n",
    "    for universe_id in range(num_universes_to_use):\n",
    "        \n",
    "        planes = planes_l[universe_id]\n",
    "        hash_value = hash_value_of_vector(v, planes)\n",
    "        hash_table = hash_tables[universe_id]\n",
    "        document_vectors_l = hash_table[hash_value]\n",
    "        id_table = id_tables[universe_id]\n",
    "        new_ids_to_consider = id_table[hash_value]\n",
    "\n",
    "        if doc_id in new_ids_to_consider:\n",
    "            new_ids_to_consider.remove(doc_id)\n",
    "            print(f\"removed doc_id {doc_id} of input vector from new_ids_to_search\")\n",
    "\n",
    "        for i, new_id in enumerate(new_ids_to_consider):\n",
    "            if new_id not in ids_to_consider_set:\n",
    "                document_vector_at_i = document_vectors_l[i]\n",
    "                \n",
    "                vecs_to_consider_l.append(document_vector_at_i)\n",
    "                ids_to_consider_l.append(new_id)\n",
    "                ids_to_consider_set.add(new_id)\n",
    "                \n",
    "    print(\"Fast considering %d vecs\" % len(vecs_to_consider_l))\n",
    "    vecs_to_consider_arr = np.array(vecs_to_consider_l)\n",
    "    nearest_neighbor_idx_l = nearest_neighbor(v, vecs_to_consider_arr, k=k)\n",
    "    print(nearest_neighbor_idx_l)\n",
    "    print(ids_to_consider_l)\n",
    "    nearest_neighbor_ids = [ids_to_consider_l[idx]\n",
    "                            for idx in nearest_neighbor_idx_l]\n",
    "\n",
    "    return nearest_neighbor_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_id = 0\n",
    "doc_to_search = all_tweets[doc_id]\n",
    "vec_to_search = document_vecs[doc_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed doc_id 0 of input vector from new_ids_to_search\n",
      "removed doc_id 0 of input vector from new_ids_to_search\n",
      "removed doc_id 0 of input vector from new_ids_to_search\n",
      "removed doc_id 0 of input vector from new_ids_to_search\n",
      "removed doc_id 0 of input vector from new_ids_to_search\n",
      "Fast considering 77 vecs\n",
      "[26  8  0]\n",
      "[51, 105, 154, 160, 195, 253, 1876, 2478, 701, 1205, 1300, 1581, 1681, 1685, 2714, 4149, 4157, 4232, 4753, 5684, 6821, 9239, 213, 339, 520, 1729, 2140, 2786, 3028, 3162, 3259, 3654, 4002, 4047, 5263, 5492, 5538, 5649, 5656, 5729, 7076, 9063, 9207, 9789, 9927, 207, 254, 1302, 1480, 1815, 2298, 2620, 2741, 3525, 3837, 4704, 4871, 5327, 5386, 5923, 6033, 6371, 6762, 7288, 7472, 7774, 7790, 7947, 8061, 8224, 8276, 8892, 9096, 9153, 9175, 9323, 9740]\n"
     ]
    }
   ],
   "source": [
    "nearest_neighbor_ids = approximate_knn(doc_id, vec_to_search, planes_l, k=3, num_universes_to_use=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbors for document 0\n",
      "Document contents: #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
      "\n",
      "Nearest neighbor at document id 2140\n",
      "document contents: @PopsRamjet come one, every now and then is not so bad :)\n",
      "Nearest neighbor at document id 701\n",
      "document contents: With the top cutie of Bohol :) https://t.co/Jh7F6U46UB\n",
      "Nearest neighbor at document id 51\n",
      "document contents: #FollowFriday @France_Espana @reglisse_menthe @CCI_inter for being top engaged members in my community this week :)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Nearest neighbors for document {doc_id}\")\n",
    "print(f\"Document contents: {doc_to_search}\")\n",
    "print(\"\")\n",
    "\n",
    "for neighbor_id in nearest_neighbor_ids:\n",
    "    print(f\"Nearest neighbor at document id {neighbor_id}\")\n",
    "    print(f\"document contents: {all_tweets[neighbor_id]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
