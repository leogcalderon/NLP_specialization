{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp import get_word_tag, preprocess\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0: Data Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"WSJ_02-21.pos\", 'r') as f:\n",
    "    training_corpus = f.readlines()\n",
    "\n",
    "with open(\"hmm_vocab.txt\", 'r') as f:\n",
    "    voc_l = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "said\tVBD\n",
      "\n",
      "it\tPRP\n",
      "\n",
      "expects\tVBZ\n",
      "\n",
      "its\tPRP$\n",
      "\n",
      "U.S.\tNNP\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Train corpus\n",
    "for i in range(5):\n",
    "    print(training_corpus[60:65][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['zeros', 'zinc', 'zip', 'zombie', 'zone', 'zones', 'zoning', '{', '}', '']\n"
     ]
    }
   ],
   "source": [
    "print(voc_l[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get an index for each word in vocab\n",
    "vocab = {}\n",
    "for i,word in enumerate(sorted(voc_l)):\n",
    "    vocab[word] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test corpus\n",
    "with open(\"WSJ_24.pos\", 'r') as f:\n",
    "    y = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = preprocess(vocab,\"test.words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Parts-of-speech tagging\n",
    "\n",
    "## Part 1.1 - Training\n",
    "\n",
    "In this section, you will find the words that are not ambiguous.\n",
    "\n",
    "For example, the word is is a verb and it is not ambiguous.\n",
    "In the WSJ corpus, $86$% of the token are unambiguous (meaning they have only one tag)\n",
    "About $14\\%$ are ambiguous (meaning that they have more than one tag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionaries(training_corpus, vocab):\n",
    "        \n",
    "    emission_counts = defaultdict(int)\n",
    "    transition_counts = defaultdict(int)\n",
    "    tag_counts = defaultdict(int)\n",
    "    \n",
    "    prev_tag = '--s--' \n",
    "    \n",
    "    for word_tag in training_corpus:\n",
    "        \n",
    "        word, tag = get_word_tag(word_tag,vocab)\n",
    "        \n",
    "        transition_counts[(prev_tag,tag)] += 1\n",
    "        emission_counts[(tag,word)] += 1\n",
    "        tag_counts[tag] += 1\n",
    "        \n",
    "        prev_tag = tag\n",
    "        \n",
    "    return emission_counts, transition_counts, tag_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "emission_counts, transition_counts, tag_counts = create_dictionaries(training_corpus, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example states: \n",
      "\tNN:132935\n",
      "\tStart:39832\n"
     ]
    }
   ],
   "source": [
    "print(\"Example states: \\n\\tNN:{0}\\n\\tStart:{1}\".format(tag_counts[\"NN\"], tag_counts[\"--s--\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'states' are the Parts-of-speech designations found in the training data. They will also be referred to as 'tags' or POS in this assignment.\n",
    "\n",
    "    - \"NN\" is noun, singular,\n",
    "    - 'NNS' is noun, plural.\n",
    "    - In addition, there are helpful tags like '--s--' which indicate a start of a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ambiguous word example: \n",
      "('RB', 'back') 304\n",
      "('VB', 'back') 20\n",
      "('RP', 'back') 84\n",
      "('JJ', 'back') 25\n",
      "('NN', 'back') 29\n",
      "('VBP', 'back') 4\n"
     ]
    }
   ],
   "source": [
    "print(\"ambiguous word example: \")\n",
    "for tup,cnt in emission_counts.items():\n",
    "    if tup[1] == 'back': print (tup, cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.2 - Testing \n",
    "\n",
    "Now you will test the accuracy of your parts-of-speech tagger using your emission_counts dictionary.\n",
    "\n",
    "- Given your preprocessed test corpus prep, you will assign a parts-of-speech tag to every word in that corpus.\n",
    "- Using the original tagged test corpus y, you will then compute what percent of the tags you got correct.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The The\tDT\n",
      "\n",
      "economy economy\tNN\n",
      "\n",
      "'s 's\tPOS\n",
      "\n",
      "temperature temperature\tNN\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for word, y_tup in zip(pre[0][:4], y[:4]):\n",
    "    print(word, y_tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_pos(prep, y, emission_counts, vocab, states):\n",
    "    \n",
    "    prep = prep[0]\n",
    "    num_correct = 0\n",
    "    all_words = set(emission_counts.keys())\n",
    "    total = len(y)\n",
    "    \n",
    "    for word, y_tup in zip(prep, y): \n",
    "\n",
    "        y_tup_l = y_tup.split()\n",
    "        \n",
    "        if len(y_tup_l) == 2:\n",
    "            true_label = y_tup_l[1]\n",
    "    \n",
    "        count_final = 0\n",
    "        pos_final = ''\n",
    "        \n",
    "        if word in vocab:\n",
    "            for pos in states:\n",
    "                \n",
    "                key = (pos,word)\n",
    "\n",
    "                if key in emission_counts:\n",
    "                    count = emission_counts[key]\n",
    "\n",
    "                    if count>count_final: \n",
    "                        count_final = count\n",
    "                        pos_final = pos\n",
    "                        \n",
    "            if pos_final == true_label:\n",
    "                num_correct += 1\n",
    "\n",
    "    accuracy = num_correct / total\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8658147899061376"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states = sorted(tag_counts.keys())\n",
    "predict_pos(pre, y, emission_counts, vocab, states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing --> Viterbi Algorithm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
