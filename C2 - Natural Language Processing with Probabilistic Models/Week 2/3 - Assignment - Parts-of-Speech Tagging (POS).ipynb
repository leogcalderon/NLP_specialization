{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp import get_word_tag, preprocess\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0: Data Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"WSJ_02-21.pos\", 'r') as f:\n",
    "    training_corpus = f.readlines()\n",
    "\n",
    "with open(\"hmm_vocab.txt\", 'r') as f:\n",
    "    voc_l = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "said\tVBD\n",
      "\n",
      "it\tPRP\n",
      "\n",
      "expects\tVBZ\n",
      "\n",
      "its\tPRP$\n",
      "\n",
      "U.S.\tNNP\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Train corpus\n",
    "for i in range(5):\n",
    "    print(training_corpus[60:65][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['zeros', 'zinc', 'zip', 'zombie', 'zone', 'zones', 'zoning', '{', '}', '']\n"
     ]
    }
   ],
   "source": [
    "print(voc_l[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get an index for each word in vocab\n",
    "vocab = {}\n",
    "for i,word in enumerate(sorted(voc_l)):\n",
    "    vocab[word] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test corpus\n",
    "with open(\"WSJ_24.pos\", 'r') as f:\n",
    "    y = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = preprocess(vocab,\"test.words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Parts-of-speech tagging\n",
    "\n",
    "## Part 1.1 - Training\n",
    "\n",
    "In this section, you will find the words that are not ambiguous.\n",
    "\n",
    "For example, the word is is a verb and it is not ambiguous.\n",
    "In the WSJ corpus, $86$% of the token are unambiguous (meaning they have only one tag)\n",
    "About $14\\%$ are ambiguous (meaning that they have more than one tag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionaries(training_corpus, vocab):\n",
    "        \n",
    "    emission_counts = defaultdict(int)\n",
    "    transition_counts = defaultdict(int)\n",
    "    tag_counts = defaultdict(int)\n",
    "    \n",
    "    prev_tag = '--s--' \n",
    "    \n",
    "    for word_tag in training_corpus:\n",
    "        \n",
    "        word, tag = get_word_tag(word_tag,vocab)\n",
    "        \n",
    "        transition_counts[(prev_tag,tag)] += 1\n",
    "        emission_counts[(tag,word)] += 1\n",
    "        tag_counts[tag] += 1\n",
    "        \n",
    "        prev_tag = tag\n",
    "        \n",
    "    return emission_counts, transition_counts, tag_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "emission_counts, transition_counts, tag_counts = create_dictionaries(training_corpus, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example states: \n",
      "\tNN:132935\n",
      "\tStart:39832\n"
     ]
    }
   ],
   "source": [
    "print(\"Example states: \\n\\tNN:{0}\\n\\tStart:{1}\".format(tag_counts[\"NN\"], tag_counts[\"--s--\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'states' are the Parts-of-speech designations found in the training data. They will also be referred to as 'tags' or POS in this assignment.\n",
    "\n",
    "    - \"NN\" is noun, singular,\n",
    "    - 'NNS' is noun, plural.\n",
    "    - In addition, there are helpful tags like '--s--' which indicate a start of a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ambiguous word example: \n",
      "('RB', 'back') 304\n",
      "('VB', 'back') 20\n",
      "('RP', 'back') 84\n",
      "('JJ', 'back') 25\n",
      "('NN', 'back') 29\n",
      "('VBP', 'back') 4\n"
     ]
    }
   ],
   "source": [
    "print(\"ambiguous word example: \")\n",
    "for tup,cnt in emission_counts.items():\n",
    "    if tup[1] == 'back': print (tup, cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.2 - Testing \n",
    "\n",
    "Now you will test the accuracy of your parts-of-speech tagger using your emission_counts dictionary.\n",
    "\n",
    "- Given your preprocessed test corpus prep, you will assign a parts-of-speech tag to every word in that corpus.\n",
    "- Using the original tagged test corpus y, you will then compute what percent of the tags you got correct.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The The\tDT\n",
      "\n",
      "economy economy\tNN\n",
      "\n",
      "'s 's\tPOS\n",
      "\n",
      "temperature temperature\tNN\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for word, y_tup in zip(pre[0][:4], y[:4]):\n",
    "    print(word, y_tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_pos(prep, y, emission_counts, vocab, states):\n",
    "    \n",
    "    prep = prep[0]\n",
    "    num_correct = 0\n",
    "    all_words = set(emission_counts.keys())\n",
    "    total = len(y)\n",
    "    \n",
    "    for word, y_tup in zip(prep, y): \n",
    "\n",
    "        y_tup_l = y_tup.split()\n",
    "        \n",
    "        if len(y_tup_l) == 2:\n",
    "            true_label = y_tup_l[1]\n",
    "    \n",
    "        count_final = 0\n",
    "        pos_final = ''\n",
    "        \n",
    "        if word in vocab:\n",
    "            for pos in states:\n",
    "                \n",
    "                key = (pos,word)\n",
    "\n",
    "                if key in emission_counts:\n",
    "                    count = emission_counts[key]\n",
    "\n",
    "                    if count>count_final: \n",
    "                        count_final = count\n",
    "                        pos_final = pos\n",
    "                        \n",
    "            if pos_final == true_label:\n",
    "                num_correct += 1\n",
    "\n",
    "    accuracy = num_correct / total\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8658147899061376"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states = sorted(tag_counts.keys())\n",
    "predict_pos(pre, y, emission_counts, vocab, states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Hidden Markov Models for POS\n",
    "## Part 2.1 Generating Matrices\n",
    "#### Creating the 'A' transition probabilities matrix\n",
    "\n",
    "The smoothing was done as follows: \n",
    "\n",
    "$$ P(t_i | t_{i-1}) = \\frac{C(t_{i-1}, t_{i}) + \\alpha }{C(t_{i-1}) +\\alpha * N}\\tag{3}$$\n",
    "\n",
    "- $N$ is the total number of tags\n",
    "- $C(t_{i-1}, t_{i})$ is the count of the tuple (previous POS, current POS) in `transition_counts` dictionary.\n",
    "- $C(t_{i-1})$ is the count of the previous POS in the `tag_counts` dictionary.\n",
    "- $\\alpha$ is a smoothing parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transition_matrix(alpha, tag_counts, transition_counts):\n",
    "    ''' \n",
    "    Input: \n",
    "        alpha: number used for smoothing\n",
    "        tag_counts: a dictionary mapping each tag to its respective count\n",
    "        transition_counts: transition count for the previous word and tag\n",
    "    Output:\n",
    "        A: matrix of dimension (num_tags,num_tags)\n",
    "    '''\n",
    "    all_tags = sorted(tag_counts.keys())\n",
    "    num_tags = len(all_tags)\n",
    "    \n",
    "    A = np.zeros((num_tags,num_tags))\n",
    "    \n",
    "    trans_keys = set(transition_counts.keys())\n",
    "    \n",
    "    for i in range(num_tags):\n",
    "        for j in range(num_tags):\n",
    "            count = 0\n",
    "            key = (all_tags[i],all_tags[j])\n",
    "            \n",
    "            if key in transition_counts:\n",
    "                count = transition_counts[key]\n",
    "                \n",
    "            count_prev_tag = tag_counts[all_tags[i]]\n",
    "            A[i,j] = (count + alpha) / (count_prev_tag + alpha*num_tags)\n",
    "\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View a subset of transition matrix A\n",
      "                  ,         --s--             .         :        CC\n",
      ",      2.052248e-08  1.231554e-04  2.052248e-08  0.000103  0.092680\n",
      "--s--  2.510541e-08  2.510541e-08  2.510541e-08  0.002561  0.056964\n",
      ".      7.601693e-05  9.299599e-01  1.773391e-04  0.000203  0.000025\n",
      ":      6.288707e-04  6.265677e-02  1.886004e-02  0.001677  0.067267\n",
      "CC     7.683662e-03  4.175880e-08  4.175880e-08  0.000292  0.000209\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.001\n",
    "A = create_transition_matrix(alpha, tag_counts, transition_counts)\n",
    "print(\"View a subset of transition matrix A\")\n",
    "A_sub = pd.DataFrame(A[5:10,5:10], index=states[5:10], columns = states[5:10] )\n",
    "print(A_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the 'B' emission probabilities matrix\n",
    "\n",
    "$$P(w_i | t_i) = \\frac{C(t_i, word_i)+ \\alpha}{C(t_{i}) +\\alpha * N}\\tag{4}$$\n",
    "\n",
    "- $C(t_i, word_i)$ is the number of times $word_i$ was associated with $tag_i$ in the training data (stored in `emission_counts` dictionary).\n",
    "- $C(t_i)$ is the number of times $tag_i$ was in the training data (stored in `tag_counts` dictionary).\n",
    "- $N$ is the number of words in the vocabulary\n",
    "- $\\alpha$ is a smoothing parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emission_matrix(alpha, tag_counts, emission_counts, vocab):\n",
    "    '''\n",
    "    Input: \n",
    "        alpha: tuning parameter used in smoothing \n",
    "        tag_counts: a dictionary mapping each tag to its respective count\n",
    "        emission_counts: a dictionary where the keys are (tag, word) and the values are the counts\n",
    "        vocab: a dictionary where keys are words in vocabulary and value is an index\n",
    "    Output:\n",
    "        B: a matrix of dimension (num_tags, len(vocab))\n",
    "    '''\n",
    "    num_tags = len(tag_counts)\n",
    "    all_tags = sorted(tag_counts.keys())\n",
    "    num_words = len(vocab)\n",
    "    \n",
    "    B = np.zeros((num_tags, num_words))\n",
    "    \n",
    "    emis_keys = set(list(emission_counts.keys()))\n",
    "\n",
    "    for i in range(num_tags):\n",
    "        for j in range(num_words):\n",
    "\n",
    "            count = 0\n",
    "            key = (all_tags[i],vocab[j])\n",
    "\n",
    "            if key in emission_counts.keys():\n",
    "                count = emission_counts[key]\n",
    "\n",
    "            count_tag = tag_counts[all_tags[i]]\n",
    "            B[i,j] = (count + alpha) / (count_tag+ alpha*num_words)\n",
    "\n",
    "    return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                725      adroitly     engineers      promoted       synergy\n",
      "--s--  2.509047e-08  2.509047e-08  2.509047e-08  2.509047e-08  2.509047e-08\n",
      "NN     7.521128e-09  7.521128e-09  7.521128e-09  7.521128e-09  2.257091e-05\n",
      "NNS    1.670013e-08  1.670013e-08  4.676203e-04  1.670013e-08  1.670013e-08\n",
      "VB     3.779036e-08  3.779036e-08  3.779036e-08  3.779036e-08  3.779036e-08\n",
      "RB     3.226454e-08  6.456135e-05  3.226454e-08  3.226454e-08  3.226454e-08\n",
      "RP     3.723317e-07  3.723317e-07  3.723317e-07  3.723317e-07  3.723317e-07\n"
     ]
    }
   ],
   "source": [
    "B = create_emission_matrix(alpha, tag_counts, emission_counts, list(vocab))\n",
    "cidx  = ['725','adroitly','engineers', 'promoted', 'synergy']\n",
    "cols = [vocab[a] for a in cidx]\n",
    "rvals =['--s--','NN','NNS', 'VB','RB','RP']\n",
    "rows = [states.index(a) for a in rvals]\n",
    "B_sub = pd.DataFrame(B[np.ix_(rows,cols)], index=rvals, columns = cidx )\n",
    "print(B_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viterbi "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
