{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import emoji\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dict(data):\n",
    "    \"\"\"\n",
    "    Output:\n",
    "        word2Ind: returns dictionary mapping the word to its index\n",
    "        Ind2Word: returns dictionary mapping the index to its word\n",
    "    \"\"\"\n",
    "    words = sorted(list(set(data)))\n",
    "    n = len(words)\n",
    "    idx = 0\n",
    "    word2Ind = {}\n",
    "    Ind2word = {}\n",
    "    \n",
    "    for k in words:\n",
    "        word2Ind[k] = idx\n",
    "        Ind2word[idx] = k\n",
    "        idx += 1\n",
    "        \n",
    "    return word2Ind, Ind2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(corpus):\n",
    "    \n",
    "    corpus = re.sub(r'[,!?;-]+', '.', corpus)\n",
    "    corpus = nltk.word_tokenize(corpus)\n",
    "    corpus = [w.lower() for w in corpus]\n",
    "    \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_windows(words,C):\n",
    "    \n",
    "    i = C\n",
    "    \n",
    "    while i < len(words) - C:\n",
    "        center_word = words[i]\n",
    "        context_words = words[(i-C):i] + words[(i+1):(i+1+C)]\n",
    "        \n",
    "        yield center_word, context_words\n",
    "        i += 1\n",
    "        \n",
    "def word_to_one_hot_vector(word, word2Ind, V):\n",
    "    \n",
    "    one_hot_vector = np.zeros(V)\n",
    "    one_hot_vector[word2Ind[word]] = 1\n",
    "    \n",
    "    return one_hot_vector\n",
    "\n",
    "def context_words_to_vector(context_words, word2Ind, V):\n",
    "    \n",
    "    context_words_vectors = [word_to_one_hot_vector(w, word2Ind, V) for w in context_words]\n",
    "    context_words_vectors = np.mean(context_words_vectors, axis=0)\n",
    "    \n",
    "    return context_words_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('shakespeare.txt') as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = process(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 63557 words in the dataset\n"
     ]
    }
   ],
   "source": [
    "print(\"There are\" ,len(data), \"words in the dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get frequency for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = nltk.FreqDist(word for word in data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'.': 9630, 'the': 1521, 'and': 1394, 'i': 1257, 'to': 1159, 'of': 1093, 'my': 857, 'that': 781, 'in': 770, 'a': 752, ...})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create index for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary:  6371\n"
     ]
    }
   ],
   "source": [
    "word2Ind, Ind2word = get_dict(data)\n",
    "V = len(word2Ind)\n",
    "print(\"Size of vocabulary: \", V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_words = []\n",
    "context_words = []\n",
    "\n",
    "for c_w, context_w in get_windows(data[:5000],2):\n",
    "    c_words.append(c_w)\n",
    "    context_words.append(context_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n"
     ]
    }
   ],
   "source": [
    "c_words_vector = word_to_one_hot_vector(c_words[0],word2Ind,V)\n",
    "i = 0\n",
    "for c_w in c_words[1:]:\n",
    "    if i%500 == 0:\n",
    "        print(i)\n",
    "    i += 1\n",
    "    c_words_vector = np.vstack((c_words_vector, word_to_one_hot_vector(c_w,word2Ind,V)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n"
     ]
    }
   ],
   "source": [
    "context_words_vector = context_words_to_vector(context_words[0],word2Ind,V)\n",
    "i = 0\n",
    "for context_w in context_words[1:]:\n",
    "    if i%500 == 0:\n",
    "        print(i)\n",
    "    i += 1\n",
    "    context_words_vector = np.vstack((context_words_vector, context_words_to_vector(context_w,word2Ind,V)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = pd.DataFrame(columns = [\"Center word\",\"Context words\",\"Context vector\",\"Word vector\"])\n",
    "matrix[\"Center word\"] = c_words\n",
    "matrix[\"Context words\"] = context_words\n",
    "matrix[\"Context vector\"] = context_words_vector\n",
    "matrix[\"Word vector\"] = c_words_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix[\"Context vector\"] = matrix[\"Context vector\"].astype(object)\n",
    "matrix[\"Word vector\"] = matrix[\"Word vector\"].astype(object)\n",
    "\n",
    "for i in range(len(c_words)):\n",
    "    matrix[\"Context vector\"][i] = context_words_vector[i]\n",
    "    matrix[\"Word vector\"][i] = c_words_vector[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Center word</th>\n",
       "      <th>Context words</th>\n",
       "      <th>Context vector</th>\n",
       "      <th>Word vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>[o, for, muse, of]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>muse</td>\n",
       "      <td>[for, a, of, fire]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>of</td>\n",
       "      <td>[a, muse, fire, .]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fire</td>\n",
       "      <td>[muse, of, ., that]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.</td>\n",
       "      <td>[of, fire, that, would]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4991</th>\n",
       "      <td>in</td>\n",
       "      <td>[and, all, war, with]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4992</th>\n",
       "      <td>war</td>\n",
       "      <td>[all, in, with, time]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4993</th>\n",
       "      <td>with</td>\n",
       "      <td>[in, war, time, for]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4994</th>\n",
       "      <td>time</td>\n",
       "      <td>[war, with, for, love]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>for</td>\n",
       "      <td>[with, time, love, of]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4996 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Center word            Context words  \\\n",
       "0              a       [o, for, muse, of]   \n",
       "1           muse       [for, a, of, fire]   \n",
       "2             of       [a, muse, fire, .]   \n",
       "3           fire      [muse, of, ., that]   \n",
       "4              .  [of, fire, that, would]   \n",
       "...          ...                      ...   \n",
       "4991          in    [and, all, war, with]   \n",
       "4992         war    [all, in, with, time]   \n",
       "4993        with     [in, war, time, for]   \n",
       "4994        time   [war, with, for, love]   \n",
       "4995         for   [with, time, love, of]   \n",
       "\n",
       "                                         Context vector  \\\n",
       "0     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "...                                                 ...   \n",
       "4991  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4992  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4993  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4994  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4995  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                            Word vector  \n",
       "0     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "1     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "2     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "3     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "4     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "...                                                 ...  \n",
       "4991  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "4992  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "4993  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "4994  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "4995  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "\n",
       "[4996 rows x 4 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The vector X will be the context words vector, and the label Y the word vector.\n",
    "    The hidden layer neuron weights are the embedding values for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 50\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(N,activation=\"relu\",input_shape=(6371,)))\n",
    "model.add(keras.layers.Dense(6371,activation=\"softmax\"))\n",
    "model.compile(optimizer=\"adam\",loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 50)                318600    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 6371)              324921    \n",
      "=================================================================\n",
      "Total params: 643,521\n",
      "Trainable params: 643,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4996 samples\n",
      "Epoch 1/5\n",
      "4996/4996 [==============================] - 29s 6ms/sample - loss: 6.6906 - accuracy: 0.1133\n",
      "Epoch 2/5\n",
      "4996/4996 [==============================] - 26s 5ms/sample - loss: 5.8244 - accuracy: 0.1141\n",
      "Epoch 3/5\n",
      "4996/4996 [==============================] - 27s 5ms/sample - loss: 5.5691 - accuracy: 0.1141\n",
      "Epoch 4/5\n",
      "4996/4996 [==============================] - 27s 5ms/sample - loss: 5.3096 - accuracy: 0.1195\n",
      "Epoch 5/5\n",
      "4996/4996 [==============================] - 27s 5ms/sample - loss: 5.0346 - accuracy: 0.1303\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7c6c0ad790>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x = context_words_vector, y = c_words_vector, batch_size=1, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons_weights = model.get_weights()[0]\n",
    "embedding = pd.DataFrame(index = Ind2word.values(), data = neurons_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>'</th>\n",
       "      <td>-0.002797</td>\n",
       "      <td>-0.125872</td>\n",
       "      <td>-0.051267</td>\n",
       "      <td>0.000278</td>\n",
       "      <td>-0.246393</td>\n",
       "      <td>-0.198441</td>\n",
       "      <td>-0.031317</td>\n",
       "      <td>-0.129861</td>\n",
       "      <td>-0.005995</td>\n",
       "      <td>-0.026461</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055330</td>\n",
       "      <td>-0.059392</td>\n",
       "      <td>-0.026764</td>\n",
       "      <td>0.070147</td>\n",
       "      <td>-0.026359</td>\n",
       "      <td>0.023660</td>\n",
       "      <td>0.013219</td>\n",
       "      <td>-0.105551</td>\n",
       "      <td>0.040585</td>\n",
       "      <td>-0.251219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>''</th>\n",
       "      <td>0.021303</td>\n",
       "      <td>0.027340</td>\n",
       "      <td>0.011226</td>\n",
       "      <td>-0.026114</td>\n",
       "      <td>-0.012046</td>\n",
       "      <td>0.011970</td>\n",
       "      <td>-0.029422</td>\n",
       "      <td>-0.028441</td>\n",
       "      <td>-0.026767</td>\n",
       "      <td>-0.028130</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025201</td>\n",
       "      <td>-0.003711</td>\n",
       "      <td>0.002647</td>\n",
       "      <td>-0.004048</td>\n",
       "      <td>-0.024559</td>\n",
       "      <td>-0.015485</td>\n",
       "      <td>-0.026980</td>\n",
       "      <td>-0.020295</td>\n",
       "      <td>0.003518</td>\n",
       "      <td>-0.021371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'after</th>\n",
       "      <td>0.024605</td>\n",
       "      <td>-0.009025</td>\n",
       "      <td>-0.023475</td>\n",
       "      <td>0.024066</td>\n",
       "      <td>-0.016474</td>\n",
       "      <td>-0.025982</td>\n",
       "      <td>0.022019</td>\n",
       "      <td>0.006287</td>\n",
       "      <td>0.012208</td>\n",
       "      <td>-0.025217</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002015</td>\n",
       "      <td>0.028351</td>\n",
       "      <td>-0.025916</td>\n",
       "      <td>-0.001299</td>\n",
       "      <td>0.017473</td>\n",
       "      <td>-0.003613</td>\n",
       "      <td>0.008847</td>\n",
       "      <td>-0.014072</td>\n",
       "      <td>-0.005086</td>\n",
       "      <td>0.017028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'agrippa</th>\n",
       "      <td>-0.022188</td>\n",
       "      <td>0.006405</td>\n",
       "      <td>0.004457</td>\n",
       "      <td>0.009550</td>\n",
       "      <td>0.005476</td>\n",
       "      <td>0.025564</td>\n",
       "      <td>0.022134</td>\n",
       "      <td>0.025026</td>\n",
       "      <td>-0.016427</td>\n",
       "      <td>-0.023332</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024278</td>\n",
       "      <td>0.011165</td>\n",
       "      <td>0.021460</td>\n",
       "      <td>0.007511</td>\n",
       "      <td>-0.004527</td>\n",
       "      <td>-0.026452</td>\n",
       "      <td>0.008314</td>\n",
       "      <td>-0.024222</td>\n",
       "      <td>-0.027805</td>\n",
       "      <td>-0.030553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'among</th>\n",
       "      <td>-0.018193</td>\n",
       "      <td>0.013651</td>\n",
       "      <td>-0.010298</td>\n",
       "      <td>-0.018836</td>\n",
       "      <td>-0.022069</td>\n",
       "      <td>-0.018958</td>\n",
       "      <td>-0.028569</td>\n",
       "      <td>0.022031</td>\n",
       "      <td>-0.017114</td>\n",
       "      <td>0.005176</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027294</td>\n",
       "      <td>-0.016926</td>\n",
       "      <td>0.021018</td>\n",
       "      <td>0.008642</td>\n",
       "      <td>0.001621</td>\n",
       "      <td>-0.003726</td>\n",
       "      <td>0.016473</td>\n",
       "      <td>-0.003428</td>\n",
       "      <td>-0.015799</td>\n",
       "      <td>-0.016705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yourself</th>\n",
       "      <td>0.000203</td>\n",
       "      <td>0.098526</td>\n",
       "      <td>-0.024501</td>\n",
       "      <td>0.009843</td>\n",
       "      <td>-0.006699</td>\n",
       "      <td>-0.044000</td>\n",
       "      <td>0.211477</td>\n",
       "      <td>-0.019456</td>\n",
       "      <td>-0.041926</td>\n",
       "      <td>0.100803</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.097633</td>\n",
       "      <td>-0.027061</td>\n",
       "      <td>0.043011</td>\n",
       "      <td>-0.082661</td>\n",
       "      <td>-0.057625</td>\n",
       "      <td>0.085134</td>\n",
       "      <td>0.052857</td>\n",
       "      <td>0.026799</td>\n",
       "      <td>-0.064474</td>\n",
       "      <td>-0.006795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yourselves</th>\n",
       "      <td>-0.006743</td>\n",
       "      <td>0.077831</td>\n",
       "      <td>0.038741</td>\n",
       "      <td>0.099715</td>\n",
       "      <td>0.115078</td>\n",
       "      <td>0.030437</td>\n",
       "      <td>0.347510</td>\n",
       "      <td>0.064781</td>\n",
       "      <td>0.109424</td>\n",
       "      <td>0.141419</td>\n",
       "      <td>...</td>\n",
       "      <td>0.282137</td>\n",
       "      <td>-0.050893</td>\n",
       "      <td>0.214145</td>\n",
       "      <td>0.063931</td>\n",
       "      <td>0.198743</td>\n",
       "      <td>0.166175</td>\n",
       "      <td>0.129876</td>\n",
       "      <td>0.134620</td>\n",
       "      <td>0.231655</td>\n",
       "      <td>0.248063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>youth</th>\n",
       "      <td>-0.000685</td>\n",
       "      <td>-0.012225</td>\n",
       "      <td>0.054813</td>\n",
       "      <td>-0.018389</td>\n",
       "      <td>0.136421</td>\n",
       "      <td>0.106493</td>\n",
       "      <td>0.133187</td>\n",
       "      <td>0.032833</td>\n",
       "      <td>0.002516</td>\n",
       "      <td>0.246192</td>\n",
       "      <td>...</td>\n",
       "      <td>0.057155</td>\n",
       "      <td>0.228972</td>\n",
       "      <td>0.403452</td>\n",
       "      <td>0.176327</td>\n",
       "      <td>0.164586</td>\n",
       "      <td>0.154797</td>\n",
       "      <td>0.152461</td>\n",
       "      <td>0.120394</td>\n",
       "      <td>0.054942</td>\n",
       "      <td>0.195388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>youthful</th>\n",
       "      <td>0.028307</td>\n",
       "      <td>0.188929</td>\n",
       "      <td>0.115141</td>\n",
       "      <td>0.056395</td>\n",
       "      <td>0.051798</td>\n",
       "      <td>0.108896</td>\n",
       "      <td>0.014609</td>\n",
       "      <td>0.167029</td>\n",
       "      <td>0.159138</td>\n",
       "      <td>0.071809</td>\n",
       "      <td>...</td>\n",
       "      <td>0.124271</td>\n",
       "      <td>0.308703</td>\n",
       "      <td>0.180502</td>\n",
       "      <td>0.314971</td>\n",
       "      <td>0.322433</td>\n",
       "      <td>0.169249</td>\n",
       "      <td>0.232650</td>\n",
       "      <td>0.065129</td>\n",
       "      <td>0.258864</td>\n",
       "      <td>0.066509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zealous</th>\n",
       "      <td>0.002477</td>\n",
       "      <td>0.007355</td>\n",
       "      <td>0.001054</td>\n",
       "      <td>-0.019508</td>\n",
       "      <td>0.005051</td>\n",
       "      <td>0.004083</td>\n",
       "      <td>0.018432</td>\n",
       "      <td>-0.009061</td>\n",
       "      <td>0.003869</td>\n",
       "      <td>-0.022450</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021696</td>\n",
       "      <td>0.025515</td>\n",
       "      <td>-0.020918</td>\n",
       "      <td>-0.014349</td>\n",
       "      <td>-0.004980</td>\n",
       "      <td>-0.019919</td>\n",
       "      <td>0.002236</td>\n",
       "      <td>-0.015819</td>\n",
       "      <td>0.007615</td>\n",
       "      <td>0.003875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6371 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0         1         2         3         4         5   \\\n",
       "'          -0.002797 -0.125872 -0.051267  0.000278 -0.246393 -0.198441   \n",
       "''          0.021303  0.027340  0.011226 -0.026114 -0.012046  0.011970   \n",
       "'after      0.024605 -0.009025 -0.023475  0.024066 -0.016474 -0.025982   \n",
       "'agrippa   -0.022188  0.006405  0.004457  0.009550  0.005476  0.025564   \n",
       "'among     -0.018193  0.013651 -0.010298 -0.018836 -0.022069 -0.018958   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "yourself    0.000203  0.098526 -0.024501  0.009843 -0.006699 -0.044000   \n",
       "yourselves -0.006743  0.077831  0.038741  0.099715  0.115078  0.030437   \n",
       "youth      -0.000685 -0.012225  0.054813 -0.018389  0.136421  0.106493   \n",
       "youthful    0.028307  0.188929  0.115141  0.056395  0.051798  0.108896   \n",
       "zealous     0.002477  0.007355  0.001054 -0.019508  0.005051  0.004083   \n",
       "\n",
       "                  6         7         8         9   ...        40        41  \\\n",
       "'          -0.031317 -0.129861 -0.005995 -0.026461  ...  0.055330 -0.059392   \n",
       "''         -0.029422 -0.028441 -0.026767 -0.028130  ...  0.025201 -0.003711   \n",
       "'after      0.022019  0.006287  0.012208 -0.025217  ...  0.002015  0.028351   \n",
       "'agrippa    0.022134  0.025026 -0.016427 -0.023332  ... -0.024278  0.011165   \n",
       "'among     -0.028569  0.022031 -0.017114  0.005176  ... -0.027294 -0.016926   \n",
       "...              ...       ...       ...       ...  ...       ...       ...   \n",
       "yourself    0.211477 -0.019456 -0.041926  0.100803  ... -0.097633 -0.027061   \n",
       "yourselves  0.347510  0.064781  0.109424  0.141419  ...  0.282137 -0.050893   \n",
       "youth       0.133187  0.032833  0.002516  0.246192  ...  0.057155  0.228972   \n",
       "youthful    0.014609  0.167029  0.159138  0.071809  ...  0.124271  0.308703   \n",
       "zealous     0.018432 -0.009061  0.003869 -0.022450  ... -0.021696  0.025515   \n",
       "\n",
       "                  42        43        44        45        46        47  \\\n",
       "'          -0.026764  0.070147 -0.026359  0.023660  0.013219 -0.105551   \n",
       "''          0.002647 -0.004048 -0.024559 -0.015485 -0.026980 -0.020295   \n",
       "'after     -0.025916 -0.001299  0.017473 -0.003613  0.008847 -0.014072   \n",
       "'agrippa    0.021460  0.007511 -0.004527 -0.026452  0.008314 -0.024222   \n",
       "'among      0.021018  0.008642  0.001621 -0.003726  0.016473 -0.003428   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "yourself    0.043011 -0.082661 -0.057625  0.085134  0.052857  0.026799   \n",
       "yourselves  0.214145  0.063931  0.198743  0.166175  0.129876  0.134620   \n",
       "youth       0.403452  0.176327  0.164586  0.154797  0.152461  0.120394   \n",
       "youthful    0.180502  0.314971  0.322433  0.169249  0.232650  0.065129   \n",
       "zealous    -0.020918 -0.014349 -0.004980 -0.019919  0.002236 -0.015819   \n",
       "\n",
       "                  48        49  \n",
       "'           0.040585 -0.251219  \n",
       "''          0.003518 -0.021371  \n",
       "'after     -0.005086  0.017028  \n",
       "'agrippa   -0.027805 -0.030553  \n",
       "'among     -0.015799 -0.016705  \n",
       "...              ...       ...  \n",
       "yourself   -0.064474 -0.006795  \n",
       "yourselves  0.231655  0.248063  \n",
       "youth       0.054942  0.195388  \n",
       "youthful    0.258864  0.066509  \n",
       "zealous     0.007615  0.003875  \n",
       "\n",
       "[6371 rows x 50 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_words = [\"man\",\"king\",\"woman\",\"queen\"]\n",
    "test = embedding.loc[test_words,:]\n",
    "pca = PCA(n_components=2)\n",
    "decomposed = pca.fit_transform(test.to_numpy())\n",
    "\n",
    "f, ax = plt.subplots(figsize=(7,7))\n",
    "\n",
    "for i in range(len(test_words)):\n",
    "    ax.scatter(decomposed[i][0],decomposed[i][1])\n",
    "    ax.annotate(test_words[i],(decomposed[i][0],decomposed[i][1]))\n",
    "\n",
    "ax.set_xlim([-1.5,1.5])\n",
    "ax.set_ylim([-1,1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
